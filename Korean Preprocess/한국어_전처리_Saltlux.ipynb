{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hi-WoF8NsAP2"
   },
   "source": [
    "# 한국어 전처리\n",
    "\n",
    "\n",
    "> 솔트룩스 AI Labs NLP파트 김성현 (seonghyunkim@saltlux.com)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaMc1sJMVuga"
   },
   "source": [
    "## 0. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3zB8rv8NCDf"
   },
   "source": [
    "한국어에서의 다양한 전처리 방식들을 실습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2stY5xZik0S"
   },
   "source": [
    "* Basic\n",
    " - 가장 기초적인 전처리\n",
    " - html tag 제거\n",
    " - 숫자 제거\n",
    " - Lowercasing\n",
    " - \"@%*=()/+ 와 같은 punctuation 제거\n",
    "* Spell check\n",
    " - 사전 기반의 오탈자 교정\n",
    " - 줄임말 원형 복원 (e.g. I'm not happy -> I am not happy)\n",
    "* Part-of-Speech\n",
    " - 형태소 분석\n",
    " - Noun, Adjective, Verb, Adverb만 학습에 사용\n",
    "* Stemming\n",
    " - 형태소 분석 이후 동사 원형 복원\n",
    "* Stopwords\n",
    " - 불용어 제거\n",
    "* Negation\n",
    " - [논문](https://dl.acm.org/doi/pdf/10.5555/2392701.2392703)\n",
    " - 부정 표현에 대한 단순화 (e.g. I'm not happy -> I'm sad)\n",
    " - 한국어에서의 적용이 어려워, 추후 추가할 예정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9SGo8FYJ2yp"
   },
   "source": [
    "먼저 실습을 위해 한국어 wikipedia 문서를 다운받도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.320Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "4hdEumzOJ_Kn",
    "outputId": "0dc5d01a-a3d9-44db-e5bb-86a6e4b14b5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   408    0   408    0     0    857      0 --:--:-- --:--:-- --:--:--   857\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "100  476M    0  476M    0     0  18.0M      0 --:--:--  0:00:26 --:--:-- 27.7M\n"
     ]
    }
   ],
   "source": [
    "!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1EcJpRTEdGVaYhbLE1otE5iCifj_kW1_4\" > /dev/null\n",
    "!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1EcJpRTEdGVaYhbLE1otE5iCifj_kW1_4\" -o wiki_20190620.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zAzPWzWVtN4"
   },
   "source": [
    "## 1. Basic Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.322Z"
    },
    "id": "bPxWFhdVhY-b"
   },
   "outputs": [],
   "source": [
    "# 한국어 위키 데이터 load\n",
    "data = open('/home/kpkim/문서/kpkim/wiki_20190620.txt', 'r', encoding='utf-8')\n",
    "lines = data.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.324Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "8nIXezslMdDC",
    "outputId": "316b92c0-0c36-4f69-eef3-885ad8a750a4"
   },
   "outputs": [],
   "source": [
    "for i in range(0, 10):\n",
    "    print(lines[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVHfKvlTKRom"
   },
   "source": [
    "한국어 문장 분리 라이브러리 중, 가장 성능이 좋은 tokenizer 중 하나인 kss를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.326Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "iFecdharYe41",
    "outputId": "a7009822-a0e2-4be3-cd86-2ce9ff5a7f8e"
   },
   "outputs": [],
   "source": [
    "!pip install kss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장 분절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.328Z"
    },
    "id": "Pe6q853nYZ32"
   },
   "outputs": [],
   "source": [
    "import kss\n",
    "\n",
    "sentence_tokenized_text = []\n",
    "for i, line in enumerate(lines):\n",
    "    if i > 100:     # 전체 wikipedia 문서는 사이즈가 크므로, 일부만 테스트.\n",
    "        break\n",
    "    line = line.strip()\n",
    "    for sent in kss.split_sentences(line):\n",
    "        sentence_tokenized_text.append(sent.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCh0BLJdLJDZ"
   },
   "source": [
    "이제 `sentence_tokenized_text`에 문장 단위로 분리된 corpus가 저장되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### punctuation 처리, 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.330Z"
    },
    "id": "8wTd_6S9W_3i"
   },
   "outputs": [],
   "source": [
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.332Z"
    },
    "id": "p25OOozUYJV6"
   },
   "outputs": [],
   "source": [
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.334Z"
    },
    "id": "RHJ2_j9kYLco"
   },
   "outputs": [],
   "source": [
    "def clean_punc(text, punct, mapping):\n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}\n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.337Z"
    },
    "id": "FFLuqnyhNBPS"
   },
   "outputs": [],
   "source": [
    "cleaned_corpus = []\n",
    "for sent in sentence_tokenized_text:\n",
    "    cleaned_corpus.append(clean_punc(sent, punct, punct_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.341Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "id": "4Im-PMTRNQJz",
    "outputId": "0d32728a-7315-45a1-b64e-6597cbc3fc8a"
   },
   "outputs": [],
   "source": [
    "for i in range(0, 10):\n",
    "    print(cleaned_corpus[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T02:02:55.515306Z",
     "start_time": "2020-11-19T02:02:55.513503Z"
    }
   },
   "source": [
    "### 정규식을 활용한 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.343Z"
    },
    "id": "fVSpOZIoYOOJ"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(texts):\n",
    "    corpus = []\n",
    "    for i in range(0, len(texts)):\n",
    "        review = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '',str(texts[i])) #remove punctuation\n",
    "        review = re.sub(r'\\d+','', str(texts[i]))# remove number\n",
    "        review = review.lower() #lower case\n",
    "        review = re.sub(r'\\s+', ' ', review) #remove extra space\n",
    "        review = re.sub(r'<[^>]+>','',review) #remove Html tags\n",
    "        review = re.sub(r'\\s+', ' ', review) #remove spaces\n",
    "        review = re.sub(r\"^\\s+\", '', review) #remove space from start\n",
    "        review = re.sub(r'\\s+$', '', review) #remove space from the end\n",
    "        corpus.append(review)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.345Z"
    },
    "id": "wwvWW5AuOIFS"
   },
   "outputs": [],
   "source": [
    "basic_preprocessed_corpus = clean_text(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.346Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "id": "8JXT1xXdOaMh",
    "outputId": "f70bf3f6-2c89-44cc-8a0c-adfe4f5c73f8"
   },
   "outputs": [],
   "source": [
    "for i in range(0, 10):\n",
    "    print(basic_preprocessed_corpus[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REdDar2RQyN5"
   },
   "source": [
    "## 2. Spell check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrkpMW2sW8if"
   },
   "source": [
    "띄어쓰기 검사로는 [한국어 띄어쓰기 검사 라이브러리](https://github.com/haven-jeon/PyKoSpacing)를 사용하고,   \n",
    "맞춤법 검사로는 [한국어 맞춤법 검사 라이브러리](https://github.com/ssut/py-hanspell)와, [논문](https://link.springer.com/chapter/10.1007/978-3-030-12385-7_3)에서 사용되었던 외래어 사전을 사용하겠습니다.   \n",
    "반복되는 이모티콘이나 자소는 이 [라이브러리](https://github.com/lovit/soynlp)를 이용해 필터링 하겠습니다.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8vzn-5TV8f6"
   },
   "source": [
    "먼저 띄어쓰기 검사기를 설치하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.349Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 899
    },
    "id": "lxlDMy8oV-oP",
    "outputId": "8089c99a-d95c-4ab8-f14d-dd807e6fdbda"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.350Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "8c1xehl2QVVd",
    "outputId": "c713a5a7-27d5-4da1-a1bb-64c71db37b7c"
   },
   "outputs": [],
   "source": [
    "from pykospacing import spacing\n",
    "spacing(\"김형호영화시장분석가는'1987'의네이버영화정보네티즌10점평에서언급된단어들을지난해12월27일부터올해1월10일까지통계프로그램R과KoNLP패키지로텍스트마이닝하여분석했다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8i2UY1EHSIcd"
   },
   "source": [
    "다음으로 맞춤법 검사기를 설치하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.353Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "8c_1H4EyWLMG",
    "outputId": "882e0907-3bf2-4420-e99c-ffce56d4a2f9"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ssut/py-hanspell.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.355Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "kOhGie5wUR3M",
    "outputId": "3acc7df7-52b0-4589-809c-457071128df2"
   },
   "outputs": [],
   "source": [
    "from hanspell import spell_checker\n",
    " \n",
    "sent = \"대체 왜 않돼는지 설명을 해바\"\n",
    "spelled_sent = spell_checker.check(sent)\n",
    "checked_sent = spelled_sent.checked\n",
    " \n",
    "print(checked_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTRPwylDYY3_"
   },
   "source": [
    "다음으로는 데이터에서 반복되는 이모티콘이나 자모를 normalization을 위한 라이브러리를 설치하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.356Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "wGfCqUaRYW2C",
    "outputId": "bb869ad0-e6b7-4560-ba8b-9018e101d783"
   },
   "outputs": [],
   "source": [
    "!pip install soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.358Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "qthNSNCeYolf",
    "outputId": "fa0b38a0-b9fb-4906-9331-02baf0a5088e"
   },
   "outputs": [],
   "source": [
    "from soynlp.normalizer import *\n",
    "print(repeat_normalize('와하하하하하하하하하핫', num_repeats=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xWaCy7ZX_R6"
   },
   "source": [
    "마지막으로 외래어 사전을 다운로드 받겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.360Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "1aGKJHKJYEk7",
    "outputId": "52aec4e0-9db5-4f67-8a50-6ee301182050"
   },
   "outputs": [],
   "source": [
    "!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1RNYpLE-xbMCGtiEHIoNsCmfcyJP3kLYn\" > /dev/null\n",
    "!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1RNYpLE-xbMCGtiEHIoNsCmfcyJP3kLYn\" -o confused_loanwords.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.362Z"
    },
    "id": "FunWsqO-Yb5H"
   },
   "outputs": [],
   "source": [
    "lownword_map = {}\n",
    "lownword_data = open('/home/kpkim/문서/kpkim/confused_loanwords.txt', 'r', encoding='utf-8')\n",
    "\n",
    "lines = lownword_data.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    miss_spell = line.split('\\t')[0]\n",
    "    ori_word = line.split('\\t')[1]\n",
    "    lownword_map[miss_spell] = ori_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.363Z"
    },
    "id": "xOcOPy82Z5Qa"
   },
   "outputs": [],
   "source": [
    "def spell_check_text(texts):\n",
    "    corpus = []\n",
    "    for sent in texts:\n",
    "        spaced_text = spacing(sent)\n",
    "        spelled_sent = spell_checker.check(sent)\n",
    "        checked_sent = spelled_sent.checked\n",
    "        normalized_sent = repeat_normalize(checked_sent)\n",
    "        for lownword in lownword_map:\n",
    "            normalized_sent = normalized_sent.replace(lownword, lownword_map[lownword])\n",
    "        corpus.append(normalized_sent)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.365Z"
    },
    "id": "wDxvxz0YbOn9"
   },
   "outputs": [],
   "source": [
    "spell_preprocessed_corpus = spell_check_text(basic_preprocessed_corpus)\n",
    "print(spell_preprocessed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agigst8ojDKo"
   },
   "source": [
    "## 3. Part-of-Speech "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ic53Jk-JjHh6"
   },
   "source": [
    "Python 기반의 형태소 분석기 중, 성능이 가장 좋은 것 중 하나인 카카오의 [Khaiii](https://github.com/kakao/khaiii)를 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.367Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Rwma8M5gjW5L",
    "outputId": "4b8acc8f-26db-45ca-c188-501197613c37"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/kakao/khaiii.git\n",
    "!pip install cmake\n",
    "!mkdir build\n",
    "!cd build && cmake /content/khaiii\n",
    "!cd /content/build/ && make all\n",
    "!cd /content/build/ && make resource\n",
    "!cd /content/build && make install\n",
    "!cd /content/build && make package_python\n",
    "!pip install /content/build/package_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.368Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wAGMx7ZnmTWW",
    "outputId": "062df4ad-acd5-4d41-d919-6db252d492db"
   },
   "outputs": [],
   "source": [
    "from khaiii import KhaiiiApi\n",
    "api = KhaiiiApi()\n",
    "\n",
    "test_sents = [\"나도 모르게 사버렸다.\", \"행복해야해!\", \"내가 안 그랬어!\", \"나는 사지 않았어.\", \"하나도 안 기쁘다.\", \"상관하지마\", \"그것 좀 가져와\"]\n",
    "\n",
    "for sent in test_sents:\n",
    "    for word in api.analyze(sent):\n",
    "        for morph in word.morphs:\n",
    "            print(morph.lex + '/' + morph.tag)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.369Z"
    },
    "id": "rVc1TUiBmM_b"
   },
   "outputs": [],
   "source": [
    "significant_tags = ['NNG', 'NNP', 'NNB', 'VV', 'VA', 'VX', 'MAG', 'MAJ', 'XSV', 'XSA']\n",
    "\n",
    "def pos_text(texts):\n",
    "    corpus = []\n",
    "    for sent in texts:\n",
    "        pos_tagged = ''\n",
    "        for word in api.analyze(sent):\n",
    "            for morph in word.morphs:\n",
    "                if morph.tag in significant_tags:\n",
    "                    pos_tagged += morph.lex + '/' + morph.tag + ' '\n",
    "        corpus.append(pos_tagged.strip())\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.371Z"
    },
    "id": "afu1QA1oq5WY"
   },
   "outputs": [],
   "source": [
    "pos_tagged_corpus = pos_text(spell_preprocessed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.372Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "id": "3fF6kFFpq_D6",
    "outputId": "27352d82-68fb-439a-81d7-08bc12421512"
   },
   "outputs": [],
   "source": [
    "for i in range(0, 30):\n",
    "    print(pos_tagged_corpus[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2er8eeAZdWEI"
   },
   "source": [
    "## 4. Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umEzIIT_u1L7"
   },
   "source": [
    "동사를 원형으로 복원하도록 하겠습니다.\n",
    "규칙은 다음과 같습니다.\n",
    "\n",
    "1. NNG|NNP|NNB + XSV|XSA --> NNG|NNP|NNB + XSV|XSA + 다\n",
    "2. NNG|NNP|NNB + XSA + VX --> NNG|NNP + XSA + 다\n",
    "3. VV --> VV + 다\n",
    "4. VX --> VX + 다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.374Z"
    },
    "id": "orvp9Jk71ind"
   },
   "outputs": [],
   "source": [
    "p1 = re.compile('[가-힣A-Za-z0-9]+/NN. [가-힣A-Za-z0-9]+/XS.')\n",
    "p2 = re.compile('[가-힣A-Za-z0-9]+/NN. [가-힣A-Za-z0-9]+/XSA [가-힣A-Za-z0-9]+/VX')\n",
    "p3 = re.compile('[가-힣A-Za-z0-9]+/VV')\n",
    "p4 = re.compile('[가-힣A-Za-z0-9]+/VX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.375Z"
    },
    "id": "mQtv-RQDAGtF"
   },
   "outputs": [],
   "source": [
    "def stemming_text(text):\n",
    "    corpus = []\n",
    "    for sent in text:\n",
    "        ori_sent = sent\n",
    "        mached_terms = re.findall(p1, ori_sent)\n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '):\n",
    "                lemma = term.split('/')[0]\n",
    "                tag = term.split('/')[-1]\n",
    "                modi_terms += lemma\n",
    "            modi_terms += '다/VV'\n",
    "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
    "        \n",
    "        mached_terms = re.findall(p2, ori_sent)\n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '):\n",
    "                lemma = term.split('/')[0]\n",
    "                tag = term.split('/')[-1]\n",
    "                if tag != 'VX':\n",
    "                    modi_terms += lemma\n",
    "            modi_terms += '다/VV'\n",
    "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
    "\n",
    "        mached_terms = re.findall(p3, ori_sent)\n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '):\n",
    "                lemma = term.split('/')[0]\n",
    "                tag = term.split('/')[-1]\n",
    "                modi_terms += lemma\n",
    "            if '다' != modi_terms[-1]:\n",
    "                modi_terms += '다'\n",
    "            modi_terms += '/VV'\n",
    "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
    "\n",
    "        mached_terms = re.findall(p4, ori_sent)\n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '):\n",
    "                lemma = term.split('/')[0]\n",
    "                tag = term.split('/')[-1]\n",
    "                modi_terms += lemma\n",
    "            if '다' != modi_terms[-1]:\n",
    "                modi_terms += '다'\n",
    "            modi_terms += '/VV'\n",
    "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
    "        corpus.append(ori_sent)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.377Z"
    },
    "id": "B0kAgk2XFfxS"
   },
   "outputs": [],
   "source": [
    "stemming_corpus = stemming_text(pos_tagged_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.379Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "id": "zXhhD9FKGw2u",
    "outputId": "fd1138dc-c046-4125-87f6-a283013cbb93"
   },
   "outputs": [],
   "source": [
    "for i in range(0, 30):\n",
    "    print(stemming_corpus[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0p2DQ8RQCi9"
   },
   "source": [
    "## 5. Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oti-xpR2QJ5P"
   },
   "source": [
    "불용어는 도메인에 맞춰서 다양하게 구축될 수 있습니다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.381Z"
    },
    "id": "4u2dbq7UdMtF"
   },
   "outputs": [],
   "source": [
    "stopwords = ['데/NNB', '좀/MAG', '수/NNB', '등/NNB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.382Z"
    },
    "id": "EmT76QMqS_e_"
   },
   "outputs": [],
   "source": [
    "def remove_stopword_text(text):\n",
    "    corpus = []\n",
    "    for sent in text:\n",
    "        modi_sent = []\n",
    "        for word in sent.split(' '):\n",
    "            if word not in stopwords:\n",
    "                modi_sent.append(word)\n",
    "        corpus.append(' '.join(modi_sent))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.385Z"
    },
    "id": "K5F1nxq-TrAi"
   },
   "outputs": [],
   "source": [
    "removed_stopword_corpus = remove_stopword_text(stemming_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T01:02:39.387Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "id": "ZAXz7TjSTzgu",
    "outputId": "6d3a1c06-ca1f-4e81-e6fc-9de300c71dc8"
   },
   "outputs": [],
   "source": [
    "for i in range(0, 30):\n",
    "    print(removed_stopword_corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9GpD1x2eT0p_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "한국어_전처리.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
